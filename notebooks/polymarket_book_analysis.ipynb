{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polymarket Book Event Analysis\n",
    "\n",
    "This notebook analyzes Polymarket market events data, specifically focusing on 'book' event types.\n",
    "We calculate the sum of the two lowest ask prices from each asset_id for each timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 polymarket market events files:\n",
      "  - 20250701_mlb-stl-pit-2025-07-01_polymarket-market-events.csv\n",
      "  - 20250701_mlb-det-wsh-2025-07-01_polymarket-market-events.csv\n",
      "  - 20250701_mlb-min-mia-2025-07-01_polymarket-market-events.csv\n",
      "  - 20250701_mlb-cin-bos-2025-07-01_polymarket-market-events.csv\n"
     ]
    }
   ],
   "source": [
    "# Define path to trading data\n",
    "data_path = \"/home/jonathanmines/Documents/code/signal_drift_project/SignalDrift/data/\"\n",
    "pattern = \"20250701*polymarket-market-events.csv\"\n",
    "\n",
    "# Get all polymarket market events files\n",
    "files = glob.glob(os.path.join(data_path, pattern))\n",
    "print(f\"Found {len(files)} polymarket market events files:\")\n",
    "for file in files[:5]:  # Show first 5 files\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "if len(files) > 5:\n",
    "    print(f\"  ... and {len(files) - 5} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14794 total rows, 0 book ask events from 20250701_mlb-stl-pit-2025-07-01_polymarket-market-events.csv\n",
      "\n",
      "Sample data structure:\n",
      "Empty DataFrame\n",
      "Columns: [market_slug, market_id, asset_id, outcome_name, event_type, price, side, size, hash, timestamp, datetime, timestamp_id]\n",
      "Index: []\n",
      "\n",
      "Unique asset_ids: 0\n",
      "Unique timestamps: 0\n"
     ]
    }
   ],
   "source": [
    "def load_and_filter_data(file_path):\n",
    "    \"\"\"\n",
    "    Load a polymarket events CSV file and filter for book events with ask side.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered dataframe with book events and ask side only\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Filter for book events and ask side only\n",
    "        book_asks = df[(df['event_type'] == 'book') & (df['side'] == 'ask')].copy()\n",
    "        \n",
    "        # Convert timestamp to datetime for easier analysis\n",
    "        book_asks['datetime'] = pd.to_datetime(book_asks['timestamp'], unit='ms')\n",
    "        book_asks['timestamp_id'] = book_asks['timestamp']\n",
    "        \n",
    "        print(f\"Loaded {len(df)} total rows, {len(book_asks)} book ask events from {os.path.basename(file_path)}\")\n",
    "        \n",
    "        return book_asks\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Test with the first file\n",
    "if files:\n",
    "    test_data = load_and_filter_data(files[0])\n",
    "    print(f\"\\nSample data structure:\")\n",
    "    print(test_data.head())\n",
    "    print(f\"\\nUnique asset_ids: {test_data['asset_id'].nunique()}\")\n",
    "    print(f\"Unique timestamps: {test_data['timestamp'].nunique()}\")\n",
    "    test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_object(group_df):\n",
    "    \"\"\"\n",
    "    For each X group, create a summary object with:\n",
    "    - sum_Y45_p: sum of minimum P values for Y=4 and Y=5\n",
    "    - min_Y{value}_p: minimum P value for each Y group\n",
    "    - min_Y{value}_N: N value corresponding to minimum P for each Y group\n",
    "    \"\"\"\n",
    "    # Group by asset_id within this timestamp group and get the row with minimum price for each asset_id\n",
    "    asset_groups = group_df.groupby('asset_id')\n",
    "    min_p_rows = asset_groups.apply(lambda x: x.loc[x['price'].idxmin()], include_groups=False)\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # Get minimum P values for each Y group\n",
    "    min_p_values = []\n",
    "    for y_val, row in min_p_rows.iterrows():\n",
    "        result['market_slug'] = row['market_slug']\n",
    "        result['datetime'] = row['datetime']\n",
    "        result['timestamp'] = row['timestamp']\n",
    "        result[f'price_{y_val[-4:]}'] = row['price']\n",
    "        result[f'size_{y_val[-4:]}'] = row['size']\n",
    "        min_p_values.append(row['price'])\n",
    "\n",
    "    result['price_sum'] = sum(min_p_values)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_lowest_ask_prices_by_timestamp(df):\n",
    "    x = df.groupby('timestamp_id').apply(create_summary_object, include_groups=False)\n",
    "    return pd.json_normalize(x)\n",
    "\n",
    "a = get_lowest_ask_prices_by_timestamp(test_data)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/signal_drift_project/SignalDrift/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'datetime'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m     plt.tight_layout()\n\u001b[32m     35\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mcreate_visualization\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mcreate_visualization\u001b[39m\u001b[34m(result_df, title_suffix)\u001b[39m\n\u001b[32m      9\u001b[39m plt.figure(figsize=(\u001b[32m14\u001b[39m, \u001b[32m8\u001b[39m))\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create the main plot\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m plt.plot(\u001b[43mresult_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdatetime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, result_df[\u001b[33m'\u001b[39m\u001b[33mprice_sum\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     13\u001b[39m          linewidth=\u001b[32m2\u001b[39m, marker=\u001b[33m'\u001b[39m\u001b[33mo\u001b[39m\u001b[33m'\u001b[39m, markersize=\u001b[32m3\u001b[39m, alpha=\u001b[32m0.8\u001b[39m)\n\u001b[32m     15\u001b[39m plt.title(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSum of Two Lowest Ask Prices Over Time\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle_suffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m, fontsize=\u001b[32m16\u001b[39m, fontweight=\u001b[33m'\u001b[39m\u001b[33mbold\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     16\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mTime\u001b[39m\u001b[33m'\u001b[39m, fontsize=\u001b[32m12\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/signal_drift_project/SignalDrift/venv/lib/python3.12/site-packages/pandas/core/frame.py:4090\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4088\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4089\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4090\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4091\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4092\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/signal_drift_project/SignalDrift/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'datetime'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_visualization(result_df, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Create a line plot showing the combined lowest ask prices over time.\n",
    "    \n",
    "    Args:\n",
    "        result_df: DataFrame with timestamp and combined_lowest_asks columns\n",
    "        title_suffix: Additional text for the plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create the main plot\n",
    "    plt.plot(result_df['datetime'], result_df['price_sum'], \n",
    "             linewidth=2, marker='o', markersize=3, alpha=0.8)\n",
    "    \n",
    "    plt.title(f'Sum of Two Lowest Ask Prices Over Time{title_suffix}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Combined Lowest Ask Prices', fontsize=12)\n",
    "    \n",
    "    # Improve x-axis formatting\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add some statistics as text\n",
    "    stats_text = f\"\"\"Data Points: {len(result_df)}\n",
    "Min: {result_df['price_sum'].min():.4f}\n",
    "Max: {result_df['price_sum'].max():.4f}\n",
    "Mean: {result_df['price_sum'].mean():.4f}\"\"\"\n",
    "    \n",
    "    plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, \n",
    "             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_visualization(a, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_files(file_list, max_files=None):\n",
    "    \"\"\"\n",
    "    Process multiple polymarket events files and combine results.\n",
    "    \n",
    "    Args:\n",
    "        file_list: List of file paths to process\n",
    "        max_files: Maximum number of files to process (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with filename as key and result DataFrame as value\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    files_to_process = file_list[:max_files] if max_files else file_list\n",
    "    \n",
    "    for i, file_path in enumerate(files_to_process):\n",
    "        print(f\"Processing file {i+1}/{len(files_to_process)}: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Load and filter data\n",
    "        book_data = load_and_filter_data(file_path)\n",
    "        \n",
    "        if not book_data.empty:\n",
    "            # Calculate lowest ask prices\n",
    "            result = get_lowest_ask_prices_by_timestamp(book_data)\n",
    "            all_results[os.path.basename(file_path)] = result\n",
    "            \n",
    "            print(f\"  -> Generated {len(result)} data points\\n\")\n",
    "        else:\n",
    "            print(f\"  -> No valid data found\\n\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Process first 3 files as an example (you can change this number or remove the limit)\n",
    "print(\"Processing multiple files...\")\n",
    "all_results = process_all_files(files, max_files=8)\n",
    "\n",
    "print(f\"\\nProcessed {len(all_results)} files successfully:\")\n",
    "for filename, result_df in all_results.items():\n",
    "    print(f\"  - {filename}: {len(result_df)} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual plots for each file\n",
    "for filename, result_df in all_results.items():\n",
    "    if not result_df.empty:\n",
    "        print(f\"\\n=== Visualization for {filename} ===\")\n",
    "        create_visualization(result_df, f\" - {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Combine all results into a single plot for comparison\n",
    "if len(all_results) > 1:f\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(all_results)))\n",
    "    \n",
    "    for i, (filename, result_df) in enumerate(all_results.items()):\n",
    "        if not result_df.empty and '20250619' in filename:\n",
    "            plt.plot(result_df['datetime'], result_df['price_sum'], \n",
    "                    linewidth=2, marker='o', markersize=2, alpha=0.7, \n",
    "                    label=filename.replace('-polymarket_market_events.csv', ''), \n",
    "                    color=colors[i])\n",
    "    \n",
    "    plt.title('Combined View: Sum of Two Lowest Ask Prices Over Time', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Combined Lowest Ask Prices', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nCombined visualization created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary analysis\n",
    "print(\"=== SUMMARY ANALYSIS ===\")\n",
    "print(f\"Total files processed: {len(all_results)}\")\n",
    "\n",
    "all_stats = []\n",
    "for filename, result_df in all_results.items():\n",
    "    if not result_df.empty:\n",
    "        stats = {\n",
    "            'filename': filename,\n",
    "            'data_points': len(result_df),\n",
    "            'min_price': result_df['price_sum'].min(),\n",
    "            'max_price': result_df['price_sum'].max(),\n",
    "            'mean_price': result_df['price_sum'].mean(),\n",
    "            'std_price': result_df['price_sum'].std(),\n",
    "        }\n",
    "        all_stats.append(stats)\n",
    "\n",
    "if all_stats:\n",
    "    summary_df = pd.DataFrame(all_stats)\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(summary_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"Total data points across all files: {summary_df['data_points'].sum()}\")\n",
    "    print(f\"Average data points per file: {summary_df['data_points'].mean():.1f}\")\n",
    "else:\n",
    "    print(\"No valid data found in any files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Order Book Analysis\n",
    "\n",
    "Now we'll analyze the synthetic order book files (`...order_book.csv`) to identify arbitrage opportunities. \n",
    "This analysis should show more opportunities than the Market Events analysis above since we capture price changes more frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all synthetic order book files\n",
    "order_book_pattern = \"20250701*synthetic-order-book.csv\"\n",
    "order_book_files = glob.glob(os.path.join(data_path, order_book_pattern))\n",
    "\n",
    "print(f\"Found {len(order_book_files)} synthetic order book files:\")\n",
    "for file in order_book_files[:5]:  # Show first 5 files\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "if len(order_book_files) > 5:\n",
    "    print(f\"  ... and {len(order_book_files) - 5} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synthetic_order_book(file_path):\n",
    "    \"\"\"\n",
    "    Load a synthetic order book CSV file and filter for ask side orders.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered dataframe with ask side orders only\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Filter for ask side only\n",
    "        ask_orders = df[df['side'] == 'ask'].copy()\n",
    "        \n",
    "        # Convert timestamp to datetime for easier analysis\n",
    "        ask_orders['datetime'] = pd.to_datetime(ask_orders['timestamp'], unit='ms')\n",
    "        ask_orders['timestamp_id'] = ask_orders['timestamp']\n",
    "        \n",
    "        print(f\"Loaded {len(df)} total rows, {len(ask_orders)} ask orders from {os.path.basename(file_path)}\")\n",
    "        \n",
    "        return ask_orders\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Test with the first file\n",
    "if order_book_files:\n",
    "    test_order_book = load_synthetic_order_book(order_book_files[0])\n",
    "    print(f\"\\nSample synthetic order book data:\")\n",
    "    print(test_order_book.head())\n",
    "    print(f\"\\nUnique asset_ids: {test_order_book['asset_id'].nunique()}\")\n",
    "    print(f\"Unique timestamps: {test_order_book['timestamp'].nunique()}\")\n",
    "    print(f\"Unique outcome_names: {test_order_book['outcome_name'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_order_book_summary(group_df):\n",
    "    \"\"\"\n",
    "    For each timestamp group, create a summary object with:\n",
    "    - sum of minimum ask prices for each asset_id\n",
    "    - minimum price and size for each asset\n",
    "    \"\"\"\n",
    "    # Group by asset_id within this timestamp group and get the row with minimum price for each asset_id\n",
    "    asset_groups = group_df.groupby('asset_id')\n",
    "    min_p_rows = asset_groups.apply(lambda x: x.loc[x['price'].idxmin()], include_groups=False)\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # Get minimum price values for each asset\n",
    "    min_p_values = []\n",
    "    for asset_id, row in min_p_rows.iterrows():\n",
    "        result['market_slug'] = row['market_slug']\n",
    "        result['datetime'] = row['datetime']\n",
    "        result['timestamp'] = row['timestamp']\n",
    "        # Use last 4 characters of asset_id as identifier\n",
    "        result[f'price_{asset_id[-4:]}'] = row['price']\n",
    "        result[f'size_{asset_id[-4:]}'] = row['size']\n",
    "        result[f'outcome_{asset_id[-4:]}'] = row['outcome_name']\n",
    "        \n",
    "        min_p_values.append(row['price'])\n",
    "\n",
    "    result['price_sum'] = sum(min_p_values)\n",
    "    return result\n",
    "\n",
    "def get_synthetic_order_book_arb_opportunities(df):\n",
    "    \"\"\"Calculate arbitrage opportunities from synthetic order book data.\"\"\"\n",
    "    x = df.groupby('timestamp_id').apply(create_order_book_summary, include_groups=False)\n",
    "    return pd.json_normalize(x)\n",
    "\n",
    "# Test with first file\n",
    "if order_book_files and not test_order_book.empty:\n",
    "    order_book_arb = get_synthetic_order_book_arb_opportunities(test_order_book)\n",
    "    print(f\"Found {len(order_book_arb)} timestamp groups\")\n",
    "    print(f\"\\nSample arbitrage data:\")\n",
    "    print(order_book_arb.head())\n",
    "    \n",
    "    # Show opportunities where sum < 1\n",
    "    opportunities = order_book_arb[order_book_arb['price_sum'] < 1.0]\n",
    "    print(f\"\\nFound {len(opportunities)} arbitrage opportunities (sum < $1)\")\n",
    "    if not opportunities.empty:\n",
    "        print(opportunities[['datetime', 'price_sum']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization for synthetic order book data\n",
    "if order_book_files and not test_order_book.empty:\n",
    "    create_visualization(order_book_arb, \" - Synthetic Order Book\")\n",
    "    \n",
    "    # Highlight arbitrage opportunities\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot all data points\n",
    "    plt.plot(order_book_arb['datetime'], order_book_arb['price_sum'], \n",
    "             linewidth=2, marker='o', markersize=3, alpha=0.6, label='Sum of Lowest Asks')\n",
    "    \n",
    "    # Highlight opportunities (sum < 1.0) in red\n",
    "    opportunities = order_book_arb[order_book_arb['price_sum'] < 1.0]\n",
    "    if not opportunities.empty:\n",
    "        plt.scatter(opportunities['datetime'], opportunities['price_sum'], \n",
    "                   color='red', s=50, zorder=5, label=f'Arbitrage Opportunities (n={len(opportunities)})')\n",
    "    \n",
    "    # Add horizontal line at y=1\n",
    "    plt.axhline(y=1.0, color='green', linestyle='--', alpha=0.7, label='Break-even threshold ($1)')\n",
    "    \n",
    "    plt.title('Synthetic Order Book: Arbitrage Opportunities', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Sum of Lowest Ask Prices', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_order_book_files(file_list, max_files=None):\n",
    "    \"\"\"\n",
    "    Process multiple synthetic order book files and combine results.\n",
    "    \n",
    "    Args:\n",
    "        file_list: List of file paths to process\n",
    "        max_files: Maximum number of files to process (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with filename as key and result DataFrame as value\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    files_to_process = file_list[:max_files] if max_files else file_list\n",
    "    \n",
    "    for i, file_path in enumerate(files_to_process):\n",
    "        print(f\"\\nProcessing file {i+1}/{len(files_to_process)}: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Load and filter data\n",
    "        order_book_data = load_synthetic_order_book(file_path)\n",
    "        \n",
    "        if not order_book_data.empty:\n",
    "            # Calculate arbitrage opportunities\n",
    "            result = get_synthetic_order_book_arb_opportunities(order_book_data)\n",
    "            all_results[os.path.basename(file_path)] = result\n",
    "            \n",
    "            # Calculate statistics\n",
    "            opportunities = result[result['price_sum'] < 1.0]\n",
    "            \n",
    "            print(opportunities[['market_slug', 'timestamp', 'price_sum']])\n",
    "                \n",
    "            print(f\"  -> Generated {len(result)} data points\")\n",
    "            print(f\"  -> Found {len(opportunities)} arbitrage opportunities ({len(opportunities)/len(result)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  -> No valid data found\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Process synthetic order book files\n",
    "print(\"Processing synthetic order book files...\")\n",
    "order_book_results = process_all_order_book_files(order_book_files, max_files=8)\n",
    "\n",
    "print(f\"\\n\\nProcessed {len(order_book_results)} synthetic order book files successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual plots for each synthetic order book file\n",
    "for filename, result_df in order_book_results.items():\n",
    "    if not result_df.empty:\n",
    "        print(f\"\\n=== Visualization for {filename} ===\")\n",
    "        \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Plot all data points\n",
    "        plt.plot(result_df['datetime'], result_df['price_sum'], \n",
    "                 linewidth=2, marker='o', markersize=3, alpha=0.6, label='Sum of Lowest Asks')\n",
    "        \n",
    "        # Highlight opportunities (sum < 1.0) in red\n",
    "        opportunities = result_df[result_df['price_sum'] < 1.0]\n",
    "        if not opportunities.empty:\n",
    "            plt.scatter(opportunities['datetime'], opportunities['price_sum'], \n",
    "                       color='red', s=50, zorder=5, label=f'Arbitrage Opportunities (n={len(opportunities)})')\n",
    "        \n",
    "        # Add horizontal line at y=1\n",
    "        plt.axhline(y=1.0, color='green', linestyle='--', alpha=0.7, label='Break-even threshold ($1)')\n",
    "        \n",
    "        plt.title(f'Synthetic Order Book: {filename}', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Time', fontsize=12)\n",
    "        plt.ylabel('Sum of Lowest Ask Prices', fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add statistics text\n",
    "        stats_text = f\"\"\"Data Points: {len(result_df)}\n",
    "Opportunities: {len(opportunities)} ({len(opportunities)/len(result_df)*100:.1f}%)\n",
    "Min Sum: {result_df['price_sum'].min():.4f}\n",
    "Max Sum: {result_df['price_sum'].max():.4f}\"\"\"\n",
    "        \n",
    "        plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, \n",
    "                 verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Market Events vs Synthetic Order Book\n",
    "\n",
    "Let's compare the arbitrage opportunities found in both data sources by overlaying the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find matching files between market events and synthetic order book\n",
    "matching_games = []\n",
    "\n",
    "for market_file in all_results.keys():\n",
    "    # Extract game identifier from market events filename\n",
    "    game_id = market_file.replace('-polymarket_market_events.csv', '')\n",
    "    \n",
    "    # Look for corresponding order book file\n",
    "    order_book_file = f\"{game_id}-order_book.csv\"\n",
    "    \n",
    "    if order_book_file in order_book_results:\n",
    "        matching_games.append((game_id, market_file, order_book_file))\n",
    "\n",
    "print(f\"Found {len(matching_games)} matching game pairs\")\n",
    "for game_id, market_file, order_book_file in matching_games:\n",
    "    print(f\"  - {game_id}\")\n",
    "\n",
    "# Create overlay plots for matching games\n",
    "for game_id, market_file, order_book_file in matching_games[:3]:  # Show first 3 matches\n",
    "    market_df = all_results[market_file]\n",
    "    order_book_df = order_book_results[order_book_file]\n",
    "    \n",
    "    if not market_df.empty and not order_book_df.empty:\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        \n",
    "        # Plot market events data\n",
    "        plt.plot(market_df['datetime'], market_df['price_sum'], \n",
    "                 linewidth=2, marker='o', markersize=4, alpha=0.7, \n",
    "                 label='Market Events (book events only)', color='blue')\n",
    "        \n",
    "        # Plot synthetic order book data\n",
    "        plt.plot(order_book_df['datetime'], order_book_df['price_sum'], \n",
    "                 linewidth=2, marker='s', markersize=3, alpha=0.6, \n",
    "                 label='Synthetic Order Book', color='orange')\n",
    "        \n",
    "        # Highlight opportunities from both sources\n",
    "        market_opps = market_df[market_df['price_sum'] < 1.0]\n",
    "        order_book_opps = order_book_df[order_book_df['price_sum'] < 1.0]\n",
    "        \n",
    "        if not market_opps.empty:\n",
    "            plt.scatter(market_opps['datetime'], market_opps['price_sum'], \n",
    "                       color='darkblue', s=80, zorder=5, marker='o',\n",
    "                       label=f'Market Events Opportunities (n={len(market_opps)})')\n",
    "        \n",
    "        if not order_book_opps.empty:\n",
    "            plt.scatter(order_book_opps['datetime'], order_book_opps['price_sum'], \n",
    "                       color='red', s=60, zorder=5, marker='s',\n",
    "                       label=f'Order Book Opportunities (n={len(order_book_opps)})')\n",
    "        \n",
    "        # Add horizontal line at y=1\n",
    "        plt.axhline(y=1.0, color='green', linestyle='--', alpha=0.7, \n",
    "                   label='Break-even threshold ($1)')\n",
    "        \n",
    "        plt.title(f'Comparison: {game_id}', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Time', fontsize=12)\n",
    "        plt.ylabel('Sum of Lowest Ask Prices', fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add comparison statistics\n",
    "        stats_text = f\"\"\"Market Events: {len(market_df)} points, {len(market_opps)} opportunities\n",
    "Synthetic Order Book: {len(order_book_df)} points, {len(order_book_opps)} opportunities\n",
    "Order Book captures {len(order_book_df)/len(market_df):.1f}x more data points\"\"\"\n",
    "        \n",
    "        plt.text(0.02, 0.02, stats_text, transform=plt.gca().transAxes, \n",
    "                 verticalalignment='bottom', \n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison statistics\n",
    "print(\"=== OVERALL COMPARISON ===\\n\")\n",
    "\n",
    "# Collect statistics for both data sources\n",
    "market_stats = []\n",
    "order_book_stats = []\n",
    "\n",
    "for game_id, market_file, order_book_file in matching_games:\n",
    "    market_df = all_results[market_file]\n",
    "    order_book_df = order_book_results[order_book_file]\n",
    "    \n",
    "    if not market_df.empty and not order_book_df.empty:\n",
    "        market_opps = market_df[market_df['price_sum'] < 1.0]\n",
    "        order_book_opps = order_book_df[order_book_df['price_sum'] < 1.0]\n",
    "        \n",
    "        market_stats.append({\n",
    "            'game': game_id,\n",
    "            'data_points': len(market_df),\n",
    "            'opportunities': len(market_opps),\n",
    "            'opp_rate': len(market_opps) / len(market_df) * 100 if len(market_df) > 0 else 0\n",
    "        })\n",
    "        \n",
    "        order_book_stats.append({\n",
    "            'game': game_id,\n",
    "            'data_points': len(order_book_df),\n",
    "            'opportunities': len(order_book_opps),\n",
    "            'opp_rate': len(order_book_opps) / len(order_book_df) * 100 if len(order_book_df) > 0 else 0\n",
    "        })\n",
    "\n",
    "if market_stats and order_book_stats:\n",
    "    market_summary = pd.DataFrame(market_stats)\n",
    "    order_book_summary = pd.DataFrame(order_book_stats)\n",
    "    \n",
    "    print(\"Market Events Summary:\")\n",
    "    print(f\"  Total data points: {market_summary['data_points'].sum()}\")\n",
    "    print(f\"  Total opportunities: {market_summary['opportunities'].sum()}\")\n",
    "    print(f\"  Average opportunity rate: {market_summary['opp_rate'].mean():.1f}%\")\n",
    "    \n",
    "    print(\"\\nSynthetic Order Book Summary:\")\n",
    "    print(f\"  Total data points: {order_book_summary['data_points'].sum()}\")\n",
    "    print(f\"  Total opportunities: {order_book_summary['opportunities'].sum()}\")\n",
    "    print(f\"  Average opportunity rate: {order_book_summary['opp_rate'].mean():.1f}%\")\n",
    "    \n",
    "    print(f\"\\nData Coverage Improvement:\")\n",
    "    print(f\"  Synthetic Order Book captures {order_book_summary['data_points'].sum() / market_summary['data_points'].sum():.1f}x more data points\")\n",
    "    print(f\"  Synthetic Order Book finds {order_book_summary['opportunities'].sum() / market_summary['opportunities'].sum():.1f}x more opportunities\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
